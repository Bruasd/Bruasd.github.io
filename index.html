<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chengxiao DONG - CV</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://kit.fontawesome.com/a076d05399.js"></script>
</head>
<body>
    <div class="container">
    <header>
        <div class="header-content">
            <img src="photo.jpg" alt="Chengxiao DONG" class="profile-photo">
            <div class="header-text">
                <h1>Chengxiao DONG</h1>
                <p>
                    <i class="fas fa-envelope"></i> 
                    <a href="mailto:12233194@mail.sustech.edu.cn">Email: 12233194@mail.sustech.edu.cn</a> 
                    ————
                    <i class="fas fa-phone"></i> TEL: +86 15653055676
                </p>
            </div>
        </div>
    </header>

        <section>
            <h2>PERSONAL STATEMENT</h2>
            <ul>
                <li><strong>Research Interests:</strong> 3D Vision, Soft Sensing, Robotic Actuators, Automation system, and Neural Network.</li>
                <li><strong>Self-summary:</strong> A rigorous, capable, and passionate research student, with solid knowledge in the field of robotics and a solid mathematical foundation, able to analyze complex knowledge confidently and clearly, with strong logic.</li>
            </ul>
        </section>

        <section>
            <h2>EDUCATION</h2>
            <div class="education-item">
                <div class="edu-left">
                    <h3>Southern University of Science and Technology, Shenzhen, China</h3>
                    <p>Master of Engineering in Intelligent Manufacturing and Robotics</p>
                    <p>School of Design</p>
                    <p>Machine Intelligence Design & Learning Lab</p>
                </div>
                <div class="edu-right">
                    <p>Sep 2022 — Present</p>
                    <p>GPA:3.3/4.00</p>
                </div>
            </div>
            <div class="education-item">
                <div class="edu-left">
                    <h3>Tongji University, Shanghai, China</h3>
                    <p>Bachelor of Engineering in Automation</p>
                    <p>Minor in Mathematics</p>
                    <p>School of Electronic and Information Engineering</p>
                    <p>CIMS Research Center</p>
                </div>
                <div class="edu-right">
                    <p>Sep 2018 — Jul 2022</p>
                    <p>GPA:4.02/5.00 (85.21/100)</p>
                </div>
            </div>
        </section>

        <section>
            <h2>RESEARCH PROJECTS & PUBLICATIONS</h2>
            <div class="project">
                <h3>Oral Care Operation Teaching System Based on Soft Force Sensing (FIRST AUTHOR, Ongoing, Targeting RAL)</h3>
                <ul>
                    <li><strong>Abstract:</strong> Effective community oral health education programs are often limited by the absence of specialized, cost-efficient tools. This study introduces a novel soft force sensor utilizing vision-based tactile sensing with ArUco markers to enhance oral hygiene education. The sensor was designed using parametric modeling and fabricated with 3D-printed TPU. Experimental validation involved ten healthy adults performing brushing tasks, demonstrating that the sensor accurately inferred six-dimensional force and torqueand achieved 98.12% classification accuracy for identifying eight brushing regions. Additionally, an adaptive binarization algorithm ensured reliable ArUco marker recognition under diverse lighting conditions (10-5000 lux). Compared to traditional rigid sensors, the proposed soft sensor offers advantages in cost, manufacturability, and waterproofing while maintaining high accuracy and robustness. These findings highlight the sensor's potential to provide real-time feedback and personalized guidance in community oral health education, facilitating more effective and scalable public health interventions.</li>
                    <li><strong>Stack:</strong> Python, JavaScript, Neural network, Soft sensing</li>
                </ul>
            </div>
            <div class="project">
                <h3>Efficient Robotic Process Automation for Complete Edentulous Patients (FIRST AUTHOR, Under review by JBE)</h3>
                <ul>
                    <li><strong>Abstract:</strong> While advantageous in full-arch implant dentistry, intraoral scanning suffers from cumulative errors during sequential scan merging, making the measurement error exceed the acceptable thresholding. This study proposes a novel "segment-match-correct" robotic process automation workflow employing 3D registration algorithms to enhance implant positioning accuracy and efficiency. Furthermore, the impact of scanning errors on the workflow was investigated by simulating four distinct noise types. The proposed method significantly reduced dentist workload while achieving a reduced linear error, surpassing conventional intraoral scan accuracy. Noise simulations confirmed the error sources.</li>
                    <li><strong>Publication:</strong> An Automated Post-Processing Workflow for Improving the Full-arch Intraoral Measurement of Implants (First Author, Under review by Journal of Bionic Engineering).</li>
                    <li><strong>Stack:</strong> Python, Point cloud, 3D registration, CAD, Robotic process automation</li>
                </ul>
            </div>
            <div class="project">
                <h3>Causes and Mitigation Strategies for Major Crowd Stampede Disasters (SECOND AUTHOR, IN PROCEEDINGS OF IWACCE 2022)</h3>
                <ul>
                    <li><strong>Abstract:</strong> Pedestrian falls in crowded public spaces can trigger disturbances and potentially escalate into dangerous stampedes. This research developed a novel fall detection system leveraging Baidu AI to extract skeletal key points from surveillance footage. A Support Vector Machine (SVM) model was trained using features derived from pedestrian bounding box dimensions and orientation to identify fall events. Experimental validation demonstrated the effectiveness of the proposed system, offering a valuable tool for early warning and prevention of crowd-related incidents in public settings.</li>
                    <li><strong>Publication:</strong> Zhao, R., Dong, C., Zhu, W., Jia, P., Ma, Y., Li, C., & Wang, Y. (2022, December). Detection model and experiments of pedestrian fall behavior in cross passages using Baidu AI. In International Workshop on Automation, Control, and Communication Engineering (IWACCE 2022) (Vol. 12492, pp. 156-161). SPIE.</li>
                    <li><strong>Stack:</strong> Python, Online AI platform, Machine learning</li>
                </ul>
            </div>
            <div class="project">
                <h3>ADAP: Adaptive & Dynamic Arc Padding for Predicting Seam Profiles in Multi-Layer Multi-Pass Robotic Welding (Participant, Submitted to AI Open)</h3>
                <ul>
                    <li><strong>Abstract:</strong> Robotic applications in Multi-Layer Multi-Pass (MLMP) welding require real-time prediction of changing seam profile geometries for dynamic path adaptation, a task that poses significant challenges. Leveraging recent advances in Artificial Intelligence for Science, this study introduces Adaptive & Dynamic Arc Padding (ADAP), a novel approach that represents weld bead profiles using image-based boundaries and primitive arc geometries defined by arc center and radius. Trained on datasets generated from Flow-3D simulations of the MLMP process, ADAP achieves high-accuracy, real-time seam profile predictions.</li>
                    <li><strong>Publication:</strong> ADAP: Adaptive & Dynamic Arc Padding for Predicting Seam Profiles in Multi-Layer Multi-Pass Robotic Welding (Participant, Submitted to AI Open).</li>
                    <li><strong>Stack:</strong> Robotic welding, Neural network</li>
                </ul>
            </div>
            <div class="project">
                <h3>Shared Control for Underwater Human-Robot Collaboration Combining Head Movement and Vocal Cord Vibration for Intention Recognition (Participant, Under review by T-ASE)</h3>
                <ul>
                    <li><strong>Abstract:</strong> This research aims to address the limitations of current underwater exoskeletons and human-robot interaction methods, which lack intuitive control and often restrict diver movement. A novel system is proposed that combines head movement tracking and vocal cord vibration analysis to recognize diver intent, freeing up their limbs for primary tasks. This research introduces a shared control paradigm, inspired by autonomous driving, to enable more intuitive and efficient collaboration between divers and underwater robots, potentially enhancing safety and efficiency in subsea operations.</li>
                    <li><strong>Stack:</strong> Shared control, Control model</li>
                </ul>
            </div>
            <div class="project">
                <h3>Imitation Learning for Flexible Robotic Manipulation with Soft Fingers (In Preparation)</h3>
                <ul>
                    <li><strong>Abstract:</strong> The project utilizes a diffusion-based policy framework to generate manipulation trajectories for robotic arms, mimicking human-like object handling. The diffusion policy is trained on a dataset consisting of human demonstration trajectories, associated force/moment feedback from soft fingers, and desired final object states. Given the current state and target object state, the system generates sequences of actions that guide the robotic arm to achieve desired manipulation goals, leveraging real-time force/moment information for refined control. The project further incorporates conveyor belts integrated onto flexible fingers, enabling advanced in-hand manipulation and object perception capabilities. This work demonstrates expertise in combining learning-based control systems with physical interaction feedback for enhanced robotic manipulation. </li>
                    <li><strong>Stack:</strong> Python, Neural network, Soft sensing, Imitation learning</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>HONORS</h2>
            <div class="honor">
                <h3>Special Funds for the Cultivation of Guangdong College Students' Scientific and Technological Innovation</h3>
                <p><strong>Abstract:</strong> As the project leader, I applied for and completed the project Oral Care Operation Teaching System Based on Visual Touch Fusion Flexible Force Tactile Perception Technology, which is part of my research Oral Care Operation Teaching System Based on Flexible Force Sensing</p>
                <p><strong>Funding Number:</strong> Climbing Program Special Funds.pdjh2023c12301.</p>
            </div>
            <div class="honor">
                <h3>2023 Excellent Master Student of Southern University of Science and Technology</h3>
            </div>
        </section>

        <section>
            <h2>TECHNICAL SKILLS</h2>
            <ul>
                <li><strong>Programming Languages:</strong> Python (Pytorch, Open3d), C/C++, JavaScript</li>
                <li><strong>3D Modeling:</strong> Fusion 360, Rhino grasshopper</li>
                <li><strong>Human-Computer Interaction:</strong> Qt, Unity</li>
            </ul>
        </section>

<section>
    <h2>REFERENCES</h2>
    <div class="reference">
        <h3><a href="https://maindl.ancorasir.com/" target="_blank">Dr. Fang Wan</a></h3>
        <p>Assistant Professor, School of Design, Southern University of Science and Technology, Shenzhen, China</p>
        <p>Email: <a href="mailto:wanf@sustech.edu.cn">wanf@sustech.edu.cn</a></p>
        <p>Scholar Profile: <a href="https://maindl.ancorasir.com/" target="_blank">https://maindl.ancorasir.com/</a></p>
    </div>
</section>
        
    </div>
</body>
</html>
